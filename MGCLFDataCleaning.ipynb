{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning:"
      ],
      "metadata": {
        "id": "WA9rik5WY4WY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OQFzIb_kYKcM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from io import StringIO\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.abspath('/content/drive/MyDrive/Datasets-Veltris/machinegenclf/subtaskB_train.jsonl'), 'rb') as f:\n",
        "    train_data = pd.read_json(f, lines=True)\n",
        "with open(os.path.abspath('/content/drive/MyDrive/Datasets-Veltris/machinegenclf/subtaskB_dev.jsonl'), 'rb') as f:\n",
        "    dev_data = pd.read_json(f, lines=True)"
      ],
      "metadata": {
        "id": "CPdyu9ikY7k0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def remove_emojis_links(df, column_name):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    link_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "\n",
        "    emojis_found = []\n",
        "    links_found = []\n",
        "    clean_column_name = column_name + '_clean'\n",
        "    df[clean_column_name] = df[column_name]\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        emojis = emoji_pattern.findall(row[column_name])\n",
        "        if emojis:\n",
        "            emojis_found.append((index, emojis))\n",
        "            clean_text = emoji_pattern.sub('', row[column_name])\n",
        "            df.at[index, clean_column_name] = clean_text\n",
        "\n",
        "        links = link_pattern.findall(row[column_name])\n",
        "        if links:\n",
        "            links_found.append((index, links))\n",
        "            clean_text = link_pattern.sub('', row[column_name])\n",
        "            df.at[index, clean_column_name] = clean_text\n",
        "\n",
        "    return emojis_found, links_found\n",
        "\n",
        "emojis, links = remove_emojis_links(train_data, 'text')"
      ],
      "metadata": {
        "id": "apPGqqc-Y-3A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emojis[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SXUJtntZN33",
        "outputId": "6282f090-909f-485a-93a2-c58e52740264"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(73, ['☰']),\n",
              " (237, ['你好', 'こんにちは', '안녕하세요']),\n",
              " (546, ['♭', '♭', '♭', '♭', '♭', '♭', '♭', '♭']),\n",
              " (941, ['♭']),\n",
              " (966, ['⏩', '⏩']),\n",
              " (1381, ['♭']),\n",
              " (1467, ['☰', '☰', '☰']),\n",
              " (1512, ['✓', '✓']),\n",
              " (1957, ['☰', '☰']),\n",
              " (2237, ['早上好', '早', '안녕'])]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data['text'].iloc[237])"
      ],
      "metadata": {
        "id": "oUQ1FaE4ZT4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYA1kvhtc9zA",
        "outputId": "a346feb7-f5be-4bcb-a870-6831ae90bf7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.25.2)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=31aae9cc5d2ee2d24f2fdca4768a0fb442b98be804611bde430d401ffc9107c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import langid\n",
        "\n",
        "def remove_http_strings(text):\n",
        "    words = text.split()\n",
        "    cleaned_words = [word for word in words if not word.startswith(\"http\")]\n",
        "    cleaned_text = ' '.join(cleaned_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_non_english(sentence):\n",
        "    non_ascii_pattern = r'[^\\x00-\\x7F]+'\n",
        "    clean_sentence = re.sub(non_ascii_pattern, '', sentence)\n",
        "\n",
        "    return clean_sentence\n",
        "\n",
        "def remove_emojis_links(df, column_name):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    link_pattern1 = re.compile(r\"\\bhttp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b\")\n",
        "    link_pattern2 = re.compile(r'\\b(?:https?://\\S+|www\\.\\S+)\\b')\n",
        "    link_pattern3 = re.compile(r'\\bwww\\.[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,}\\b')\n",
        "    link_pattern4 = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "    # non_english_pattern = re.compile(r'name=\\w+;size=\\d+', re.U)\n",
        "    non_english_pattern = re.compile(u'[^\\u0000-\\u007F]+')\n",
        "\n",
        "    rep_specichar_pattern = re.compile(r'(?:([^a-zA-Z0-9\\s])\\1{2,})+|\\n{3,}')\n",
        "\n",
        "    emojis_found = []\n",
        "    links_found = []\n",
        "    otherlangs_found = []\n",
        "    rep_chars_found = []\n",
        "\n",
        "    clean_column_name = column_name + '_clean'\n",
        "    df[clean_column_name] = df[column_name]\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "      text = row[column_name]\n",
        "\n",
        "      emojis = emoji_pattern.findall(text)\n",
        "      if emojis:\n",
        "          emojis_found.append((index, emojis))\n",
        "          clean_text = emoji_pattern.sub('', text)\n",
        "          text = clean_text\n",
        "\n",
        "      links1 = link_pattern1.findall(text)\n",
        "      if links1:\n",
        "          links_found.append((index, links1))\n",
        "          clean_text = link_pattern1.sub('', text)\n",
        "          text = clean_text\n",
        "\n",
        "      links2 = link_pattern2.findall(text)\n",
        "      if links2:\n",
        "          links_found.append((index, links2))\n",
        "          clean_text = link_pattern2.sub('', text)\n",
        "          text = clean_text\n",
        "\n",
        "      links3 = link_pattern3.findall(text)\n",
        "      if links3:\n",
        "          links_found.append((index, links3))\n",
        "          clean_text = link_pattern3.sub('', text)\n",
        "          text = clean_text\n",
        "\n",
        "      links4 = link_pattern4.findall(text)\n",
        "      if links4:\n",
        "          links_found.append((index, links4))\n",
        "          clean_text = link_pattern3.sub('', text)\n",
        "          text = clean_text\n",
        "\n",
        "      other_lang = non_english_pattern.findall(text)\n",
        "      if other_lang:\n",
        "        otherlangs_found.append((index, other_lang))\n",
        "        clean_text = non_english_pattern.sub('', text)\n",
        "        text = clean_text\n",
        "\n",
        "      rep_chars = rep_specichar_pattern.findall(text)\n",
        "      if rep_chars:\n",
        "        rep_chars_found.append((index, other_lang))\n",
        "        clean_text = rep_specichar_pattern.sub('', text)\n",
        "        text = clean_text\n",
        "\n",
        "      text = remove_non_english(text)\n",
        "      text = remove_http_strings(text)\n",
        "      df.at[index, clean_column_name] = text\n",
        "\n",
        "    return emojis_found, links_found, otherlangs_found, rep_chars_found\n",
        "\n",
        "\n",
        "train_emojis, train_links, train_otherlang, train_repchars = remove_emojis_links(train_data, 'text')\n",
        "dev_emojis, dev_links, dev_otherlang, dev_repchars = remove_emojis_links(dev_data, 'text')"
      ],
      "metadata": {
        "id": "eTXli-l6ZW8C"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train links len: \",len(train_links))\n",
        "print(\"dev links len: \", len(dev_links))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiRFhvjAsOrY",
        "outputId": "ea0b05ae-1487-491e-d135-5a6612a8feba"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train links len:  1748\n",
            "dev links len:  64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### checking if there are any other special chars/links:"
      ],
      "metadata": {
        "id": "JSXrROBO-yjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def remove_emojis_links(df, column_name):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    link_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "\n",
        "    emojis_found = []\n",
        "    links_found = []\n",
        "    clean_column_name = column_name + '_clean'\n",
        "    df[clean_column_name] = df[column_name]\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        emojis = emoji_pattern.findall(row[column_name])\n",
        "        if emojis:\n",
        "            emojis_found.append((index, emojis))\n",
        "            clean_text = emoji_pattern.sub('', row[column_name])\n",
        "            df.at[index, clean_column_name] = clean_text\n",
        "\n",
        "        links = link_pattern.findall(row[column_name])\n",
        "        if links:\n",
        "            links_found.append((index, links))\n",
        "            clean_text = link_pattern.sub('', row[column_name])\n",
        "            df.at[index, clean_column_name] = clean_text\n",
        "\n",
        "    return emojis_found, links_found\n",
        "\n",
        "train_emojis, train_links = remove_emojis_links(train_data, 'text_clean')\n",
        "dev_emojis, dev_links = remove_emojis_links(dev_data, 'text_clean')"
      ],
      "metadata": {
        "id": "jtBsYajs-xR1"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_emojis)\n",
        "print(train_links)\n",
        "print(dev_links)\n",
        "print(dev_emojis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oFnqeEJ-9lB",
        "outputId": "afd3712e-1205-47d2-a49a-7ce0479bdb4b"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[(8388, ['http://nraas.wikis']), (27708, ['http://_URL_5_']), (45683, ['http://en.wikipedia.org/wiki/Philippine_Underwater_Hockey_Confederation']), (66152, ['https://training.adobe.com/certification/exams.html'])]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_links))\n",
        "print(len(dev_links))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr012P8wr7us",
        "outputId": "72b80464-fdd7-48d9-a966-2c65603a9700"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.at[8388, 'text_clean'] = str(train_data.text_clean.iloc[8388]).replace('Modswikiathttp://nraas.wikis paces.com /MasterController.', '')\n",
        "train_data.at[27708, 'text_clean'] = str(train_data.text_clean.iloc[27708]).replace('^1http://_URL_5_', '')\n",
        "train_data.at[45683, 'text_clean'] = str(train_data.text_clean.iloc[45683]).replace('alsohttp://en.wikipedia.org/wiki/Philippine_Underwater_Hockey_Confederation', '')\n",
        "train_data.at[66152, 'text_clean'] = str(train_data.text_clean.iloc[66152]).replace('tohttps://training.adobe.com/certification/exams.html#p=2.', '')"
      ],
      "metadata": {
        "id": "x2eo26q1ssVe"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = train_data.drop(['text'], axis=1)\n",
        "dev_clean = dev_data.drop(['text'], axis=1)\n",
        "\n",
        "train_clean.rename(columns = {'text_clean':'text'}, inplace = True)\n",
        "dev_clean.rename(columns = {'text_clean':'text'}, inplace = True)\n",
        "\n",
        "with open(os.path.abspath('/content/drive/MyDrive/Datasets-Veltris/machinegenclf/subtaskB_train_clean.jsonl'), 'w') as f:\n",
        "    f.write(train_clean.to_json(orient='records', lines=True, force_ascii=False))\n",
        "with open(os.path.abspath('/content/drive/MyDrive/Datasets-Veltris/machinegenclf/subtaskB_dev_clean.jsonl'), 'w') as f:\n",
        "    f.write(dev_clean.to_json(orient='records', lines=True, force_ascii=False))"
      ],
      "metadata": {
        "id": "Zz3uLlSogIhr"
      },
      "execution_count": 152,
      "outputs": []
    }
  ]
}