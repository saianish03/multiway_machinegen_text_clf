# End-to-End Inference Optimized AI Text Detection

This is an End-to-End project from Data to Deployment and Performance Optimization, designed to classify text based on its origin, whether it's human-written or generated by a specific language model. The classification task leverages the best model chosen after testing multiple SLMs (BERT, RoBERTa, DistilBERT, ALBERT) and LLMs (Llama 2 7B) to accurately identify the source, enhancing our understanding of the nuances in AI-generated content.

Hereâ€™s a fun twist: All the texts being classified come from LLMs, but the model doing the classification is a Small Language Model (SLM) :)


## Task Description
Given a full-text input, this project determines the source of the content. The possible classifications include:

- **Human-Written** - `0`
- **ChatGPT** - `1`
- **Cohere** - `2`
- **Davinci** - `3`
- **Dolly** - `4`
- **Bloomz** - `5`

This project is based on a problem statement from **SemEval 2024 - Task 8, Subtask B**. Although inspired by the competition, it was developed independently and was not submitted as part of the official contest. For more details on the original task, visit [this link](https://github.com/mbzuai-nlp/SemEval2024-task8).

## Model Performance

Detailed performance metrics and model specifics can be found in the [Model README](model_training/README.md).

## Model Optimization
This model was further optimized using **ONNX Runtime**, **decreasing CPU inference time by 60%**. Check out the scripts in [onnx_optimization](onnx_optimization/) to see how I utilized ONNX Runtime for this model.

## Model Deployment
The Gradio app is also deployed in an AWS EC2 instance, using Docker containerization. Checkout [main](main/) to see the Gradio app and Dockerfile.

## Demonstration

Check out the [Huggingface Spaces Gradio App](https://huggingface.co/spaces/Sansh2003/subtaskB-gradio-app) for a live demonstration of the model in action.
