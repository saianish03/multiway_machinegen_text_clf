{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ea373f-f365-489f-a9db-0f98617f00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig, RobertaTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import torch \n",
    "from peft import LoraConfig, get_peft_model\n",
    "import transformers \n",
    "import os \n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/yo\"\n",
    "from datasets import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, set_peft_model_state_dict\n",
    "# import wandb\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"merged-roberta\" # name your W&B project \n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" # log all model checkpoints\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7780db30-54de-4607-86ce-1a0527236ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34111949319826784 1.5546760583025316 1.6298069893259248 1.5542424876246252 1.8580180030005 1.5871066495799515\n"
     ]
    }
   ],
   "source": [
    "def calc_weights(total, nclass):\n",
    "    weight = total/(nclass*6)\n",
    "    return weight\n",
    "\n",
    "c0 = calc_weights(133755, 65351)\n",
    "c1 = calc_weights(133755, 14339)\n",
    "c2 = calc_weights(133755, 13678)\n",
    "c3 = calc_weights(133755, 14343)\n",
    "c4 = calc_weights(133755, 11998)\n",
    "c5 = calc_weights(133755, 14046)\n",
    "\n",
    "print(c0, c1, c2, c3, c4, c5)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "def wce_loss(logits, labels):\n",
    "    class_weights = torch.tensor([c0, c1, c2, c3, c4, c5]).to(device)\n",
    "    loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    return loss_func(logits, labels).to(device)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "#         labels = labels.to(device)\n",
    "        outputs = model(**inputs)\n",
    "#         outputs = outputs.to(device)\n",
    "        logits = outputs.logits.view(-1, self.model.config.num_labels)\n",
    "#         logits = logits.to(device)\n",
    "        loss = wce_loss(logits, labels)\n",
    "#         loss = loss.to(device)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87149252-720f-46a5-8a37-fe6f5dfda56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True, max_length=512, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    results = {}\n",
    "    results.update(f1_metric.compute(predictions=predictions, references = labels, average=\"micro\"))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f97b656-3c66-4040-a623-bbd4657df3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'human', 1: 'chatGPT', 2: 'cohere', 3: 'davinci', 4: 'bloomz', 5: 'dolly'}\n",
    "label2id = {'human': 0, 'chatGPT': 1,'cohere': 2, 'davinci': 3, 'bloomz': 4, 'dolly': 5}\n",
    "test_path =  \"/workspace/merged_dataset/subtaskB_test.jsonl\"\n",
    "test_df = pd.read_json(test_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8cca18-a9ea-4d1a-a14a-cc0ec9011152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_path, test_path, val_path,random_seed):\n",
    "    \"\"\"\n",
    "    function to read dataframe with columns\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    val_df = pd.read_json(val_path, lines=True)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c36bb38f-be4f-4616-a0e6-f204ed51e0de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4741/1438207522.py:6: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  train_df = pd.read_json(train_path, lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     10\u001b[0m set_seed(random_seed)\n\u001b[0;32m---> 12\u001b[0m train_df, valid_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# pandas dataframe to huggingface Dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(train_df)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(train_path, test_path, val_path, random_seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(train_path, test_path, val_path,random_seed):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    function to read dataframe with columns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(test_path, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     val_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(val_path, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py:1023\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1022\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m     )\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1409\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "random_seed = 0\n",
    "train_path =  \"/workspace/merged_dataset/subtaskB_train_merged.jsonl\" # For example 'subtaskA_train_multilingual.jsonl'\n",
    "test_path =  \"/workspace/merged_dataset/subtaskB_test.jsonl\" # For example 'subtaskA_test_multilingual.jsonl'\n",
    "val_path = \"/workspace/merged_dataset/subtaskB_dev.jsonl\"\n",
    "\n",
    "model =  \"roberta-large\"\n",
    "checkpoints_path = f\"/workspace/Models/{model}/\"\n",
    "model_name = model\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "train_df, valid_df, test_df = get_data(train_path, test_path, val_path, random_seed)\n",
    "\n",
    "# pandas dataframe to huggingface Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "\n",
    "\n",
    "# get tokenizer and model from huggingface\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   checkpoints_path, num_labels=len(label2id), id2label=id2label, label2id=label2id#, quantization_config=bnb_config, device_map={\"\":0}    # put your model here\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints_path)    # put your model here\n",
    "\n",
    "\n",
    "# tokenize data for train/valid\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d4cc4e-d58c-4b4f-90fe-3dec95fe5187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2699c1a5a14caabfaba3b4067996e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9926127527216174,\n",
       "  'recall': 0.851,\n",
       "  'f1-score': 0.916367552045944,\n",
       "  'support': 3000.0},\n",
       " '1': {'precision': 0.5836901518100428,\n",
       "  'recall': 0.9996666666666667,\n",
       "  'f1-score': 0.7370361268124846,\n",
       "  'support': 3000.0},\n",
       " '2': {'precision': 0.9927884615384616,\n",
       "  'recall': 0.2753333333333333,\n",
       "  'f1-score': 0.4311064718162839,\n",
       "  'support': 3000.0},\n",
       " '3': {'precision': 0.6800119510008963,\n",
       "  'recall': 0.7586666666666667,\n",
       "  'f1-score': 0.7171892232550812,\n",
       "  'support': 3000.0},\n",
       " '4': {'precision': 0.9021968101113451,\n",
       "  'recall': 0.9993333333333333,\n",
       "  'f1-score': 0.9482840423849439,\n",
       "  'support': 3000.0},\n",
       " '5': {'precision': 0.9705882352941176,\n",
       "  'recall': 0.902,\n",
       "  'f1-score': 0.93503800967519,\n",
       "  'support': 3000.0},\n",
       " 'accuracy': 0.7976666666666666,\n",
       " 'macro avg': {'precision': 0.8536480604127469,\n",
       "  'recall': 0.7976666666666666,\n",
       "  'f1-score': 0.7808369043316546,\n",
       "  'support': 18000.0},\n",
       " 'weighted avg': {'precision': 0.8536480604127468,\n",
       "  'recall': 0.7976666666666666,\n",
       "  'f1-score': 0.7808369043316546,\n",
       "  'support': 18000.0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/workspace/Models/roberta-large/checkpoint-33440\"\n",
    "\n",
    "# load tokenizer from saved model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# load best model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "#     model.add_adapter(config, \"lora\")\n",
    "\n",
    "# model.load_state_dict(torch.load(checkpoints_path+'/best'+'/model.pt'))\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# get logits from predictions and evaluate results using classification report\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "prob_pred = softmax(predictions.predictions, axis=-1)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric = evaluate.load(\"bstrai/classification_report\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e937fa43-63f5-408c-8ffc-94e029c17391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f31d9ddada48268df67924f2f3783c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9944710652414301,\n",
       "  'recall': 0.8993333333333333,\n",
       "  'f1-score': 0.944512515315946,\n",
       "  'support': 3000.0},\n",
       " '1': {'precision': 0.6525239338555265,\n",
       "  'recall': 0.9996666666666667,\n",
       "  'f1-score': 0.7896261190100052,\n",
       "  'support': 3000.0},\n",
       " '2': {'precision': 0.9955290611028316,\n",
       "  'recall': 0.44533333333333336,\n",
       "  'f1-score': 0.6153846153846154,\n",
       "  'support': 3000.0},\n",
       " '3': {'precision': 0.7530364372469636,\n",
       "  'recall': 0.806,\n",
       "  'f1-score': 0.7786185799388182,\n",
       "  'support': 3000.0},\n",
       " '4': {'precision': 0.9362898188632105,\n",
       "  'recall': 0.9993333333333333,\n",
       "  'f1-score': 0.9667849080941632,\n",
       "  'support': 3000.0},\n",
       " '5': {'precision': 0.978882833787466,\n",
       "  'recall': 0.958,\n",
       "  'f1-score': 0.9683288409703504,\n",
       "  'support': 3000.0},\n",
       " 'accuracy': 0.8512777777777778,\n",
       " 'macro avg': {'precision': 0.8851221916829047,\n",
       "  'recall': 0.8512777777777778,\n",
       "  'f1-score': 0.8438759297856498,\n",
       "  'support': 18000.0},\n",
       " 'weighted avg': {'precision': 0.8851221916829047,\n",
       "  'recall': 0.8512777777777778,\n",
       "  'f1-score': 0.8438759297856498,\n",
       "  'support': 18000.0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/workspace/Models/roberta-large/checkpoint-41800\"\n",
    "\n",
    "# load tokenizer from saved model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# load best model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "#     model.add_adapter(config, \"lora\")\n",
    "\n",
    "# model.load_state_dict(torch.load(checkpoints_path+'/best'+'/model.pt'))\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# get logits from predictions and evaluate results using classification report\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "prob_pred = softmax(predictions.predictions, axis=-1)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric = evaluate.load(\"bstrai/classification_report\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7d7900f-5d0b-4fd2-b11b-8a80a33eeb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /workspace/Models/roberta-large/checkpoint-25080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
