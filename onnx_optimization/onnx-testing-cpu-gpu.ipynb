{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e388b0a-4802-4cc7-986a-69e7cf9893ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import sys\n",
    "\n",
    "data = load_dataset(\"Sansh2003/subtask-b-examples-test\")\n",
    "data = data['test']\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aaef769-b479-4e64-81b5-10587bbc5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>model</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision-making is one of life's most difficul...</td>\n",
       "      <td>4</td>\n",
       "      <td>bloomz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When you think of voting for president, do you...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driverless Cars - Limitations &amp; Potential Draw...</td>\n",
       "      <td>4</td>\n",
       "      <td>bloomz</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are multiple benefits to attending high ...</td>\n",
       "      <td>5</td>\n",
       "      <td>dolly</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Electoral College, also known as the Presi...</td>\n",
       "      <td>5</td>\n",
       "      <td>dolly</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label   model  id\n",
       "0  Decision-making is one of life's most difficul...      4  bloomz   0\n",
       "1  When you think of voting for president, do you...      0   human   1\n",
       "2  Driverless Cars - Limitations & Potential Draw...      4  bloomz   2\n",
       "3  There are multiple benefits to attending high ...      5   dolly   3\n",
       "4  The Electoral College, also known as the Presi...      5   dolly   4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb9acbc-2384-48f4-8990-c056b3588c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Sansh2003/roberta-large-merged-subtaskB\"\n",
    "id2label = {0: 'human', 1: 'chatGPT', 2: 'cohere', 3: 'davinci', 4: 'bloomz', 5: 'dolly'}\n",
    "label2id = {'human': 0, 'chatGPT': 1,'cohere': 2, 'davinci': 3, 'bloomz': 4, 'dolly': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3360d0e1-b715-4eb4-b353-8f17ed3ad1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sreeramagiri.s/onnx_test/.onnx_multiway/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels = len(id2label), id2label=id2label, label2id=label2id)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d91aa435-617b-47a4-847b-fa0589a7c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sreeramagiri.s/onnx_test/.onnx_multiway/lib/python3.9/site-packages/torch/onnx/utils.py:2095: UserWarning: Provided key start for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  ./onnx_models/roberta-large-subtaskB-onnx.onnx\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\".\", \"onnx_models\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "export_model_path = os.path.join(output_dir, 'roberta-large-subtaskB-onnx.onnx')\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def preprocess_single_text(text, tokenizer):\n",
    "    inputs = tokenizer(text, truncation=True, max_length=512, padding=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# Get the first example data to run the model and export it to ONNX\n",
    "x = preprocess_single_text(data.iloc[0]['text'], tokenizer)\n",
    "inputs = {\n",
    "    'input_ids':      x['input_ids'].to(device),\n",
    "    'attention_mask': x['attention_mask'].to(device),\n",
    "}\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "if not os.path.exists(export_model_path):\n",
    "    with torch.no_grad():\n",
    "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "        torch.onnx.export(model,                                            # model being run\n",
    "                          args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
    "                          f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                          opset_version=11,                                 # the ONNX version to export the model to\n",
    "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                          input_names=['input_ids',                         # the model's input names\n",
    "                                       'input_mask'],\n",
    "                          output_names=['class'],                           # the model's output names\n",
    "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                        'input_mask' : symbolic_names,\n",
    "                                        'start' : symbolic_names})\n",
    "        print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "433131fa-3f42-461d-bfcb-79971b551083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cpu Inference time = 654.46 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
    "latency = []\n",
    "\n",
    "x = preprocess_single_text(data.iloc[i]['text'], tokenizer)\n",
    "total_samples = 100\n",
    "with torch.no_grad():\n",
    "    for i in range(total_samples):\n",
    "        x = preprocess_single_text(data.iloc[i]['text'], tokenizer)\n",
    "        inputs = {\n",
    "            'input_ids':      x['input_ids'].to(device),\n",
    "            'attention_mask': x['attention_mask'].to(device),\n",
    "        }\n",
    "        start = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        latency.append(time.time() - start)\n",
    "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a6d13f0-d9ab-45f7-ad06-f8b6b3b8e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 20:42:49.499093083 [W:onnxruntime:, inference_session.cc:1732 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnnxRuntime cpu Inference time = 673.88 ms\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_cpu.onnx\")\n",
    "sess_options.intra_op_num_threads = 4  # Number of threads for intra-op parallelism\n",
    "sess_options.inter_op_num_threads = 1  # Number of threads for inter-op parallelism\n",
    "\n",
    "# Specify providers when you use onnxruntime-gpu for CPU inference.\n",
    "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "latency = []\n",
    "for i in range(total_samples):\n",
    "    x = preprocess_single_text(data.iloc[i]['text'], tokenizer)\n",
    "    inputs = {\n",
    "        'input_ids':      x['input_ids'].to(device).numpy(),\n",
    "        'input_mask': x['attention_mask'].to(device).numpy(),\n",
    "    }\n",
    "    start = time.time()\n",
    "    ort_outputs = session.run(None, inputs)\n",
    "    latency.append(time.time() - start)\n",
    "print(\"OnnxRuntime cpu Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a753ef26-5992-49ca-acc2-889634c10fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Verifying correctness *****\n",
      "PyTorch and ONNX Runtime output 0 are close: True\n",
      "PyTorch and ONNX Runtime output 1 are close: True\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Verifying correctness *****\")\n",
    "for i in range(2):\n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(ort_outputs[0][0][i], outputs[0][0][i].cpu(), rtol=1e-05, atol=1e-04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e67c63c-a81e-4373-8c10-f4b2a7c38e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'davinci': 0.99892258644104, 'chatGPT': 0.0010614646598696709, 'dolly': 7.130848644010257e-06, 'human': 4.238926976540824e-06, 'cohere': 3.654642341643921e-06, 'bloomz': 9.734809509609477e-07}\n"
     ]
    }
   ],
   "source": [
    "logits = ort_outputs[0]\n",
    "\n",
    "probs = np.exp(logits.squeeze()) / np.sum(np.exp(logits.squeeze()), axis=-1)  # Apply softmax to get probabilities\n",
    "prob_dict = {}\n",
    "\n",
    "for i, k in enumerate(label2id.keys()):\n",
    "    prob_dict[k] = probs[i]\n",
    "\n",
    "prob_dict = {k: float(v) for k, v in sorted(prob_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "print(prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adddb01-956c-4bec-b6f5-cd0939f67e94",
   "metadata": {},
   "source": [
    "### GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bef98983-0fe0-4a53-9f3b-8883917504c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Model exported at  ./onnx_models/roberta-large-subtaskB-onnx-gpu.onnx\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\".\", \"onnx_models\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "export_model_path = os.path.join(output_dir, 'roberta-large-subtaskB-onnx-gpu.onnx')\n",
    "\n",
    "import torch\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Get the first example data to run the model and export it to ONNX\n",
    "x = preprocess_single_text(data.iloc[0]['text'], tokenizer)\n",
    "inputs = {\n",
    "    'input_ids':      x['input_ids'].to(device),\n",
    "    'input_mask': x['attention_mask'].to(device),\n",
    "}\n",
    "\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "if not os.path.exists(export_model_path):\n",
    "    with torch.no_grad():\n",
    "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "        torch.onnx.export(model,                                            # model being run\n",
    "                          args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
    "                          f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                          opset_version=11,                      # the ONNX version to export the model to\n",
    "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                          input_names=['input_ids',                         # the model's input names\n",
    "                                       'input_mask'],\n",
    "                          output_names=['class'],                           # the model's output names\n",
    "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                        'input_mask' : symbolic_names,\n",
    "                                        'class' : symbolic_names,\n",
    "                                        })\n",
    "        print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e332067-ec2e-4fe5-b94e-0b39312b8543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./onnx_models/roberta-large-subtaskB-onnx-gpu.onnx'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec02dc39-90d7-4c89-bfb5-a799b1047abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./onnx_models/roberta-large-subtaskB-onnx_gpu_fp16.onnx'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_fp16_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ac7648cd-c509-4273-ba1b-373420fdc9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-16 21:42:44.838328055 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31463, index: 0, mask: {4, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.838333611 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31467, index: 4, mask: {6, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.838338207 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31466, index: 3, mask: {10, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.838335196 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31464, index: 1, mask: {8, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.838504777 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31468, index: 5, mask: {2, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.893249905 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31470, index: 7, mask: {20, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.897238135 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31473, index: 10, mask: {22, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.893337671 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31472, index: 9, mask: {26, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.893316462 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31471, index: 8, mask: {24, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.913253211 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31476, index: 13, mask: {1, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.917235508 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31478, index: 15, mask: {9, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.913282581 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31477, index: 14, mask: {5, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.917267626 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31479, index: 16, mask: {13, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939811784 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31481, index: 18, mask: {7, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939799966 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31480, index: 17, mask: {11, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939821928 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31482, index: 19, mask: {3, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939831888 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31483, index: 20, mask: {17, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939841945 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31484, index: 21, mask: {21, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939851989 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31485, index: 22, mask: {25, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939862081 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31486, index: 23, mask: {27, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939871932 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31487, index: 24, mask: {23, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.939881788 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31488, index: 25, mask: {19, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:44.981251680 [E:onnxruntime:Default, env.cc:254 ThreadMain] pthread_setaffinity_np failed for thread: 31489, index: 26, mask: {15, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2025-01-16 21:42:47.006898941 [E:onnxruntime:Default, provider_bridge_ort.cc:1480 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1193 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "\n",
      "2025-01-16 21:42:47.006936681 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:747 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "               apply: Fused LayerNormalization: 49\n",
      "               apply: Fused Gelu: 24\n",
      "               apply: Fused SkipLayerNormalization: 48\n",
      "               apply: Fused Attention: 24\n",
      "         prune_graph: Removed 5 nodes\n",
      "               apply: Fused Reshape: 1\n",
      "         prune_graph: Removed 3 nodes\n",
      "remove_useless_reshape_nodes: Remove reshape node /roberta/Reshape since its input shape is same as output: [2]\n",
      "               apply: Fused BiasGelu: 24\n",
      "               apply: Fused SkipLayerNormalization(add bias): 48\n",
      "            optimize: opset version: 11\n",
      "get_operator_statistics: Operators:[('MatMul', 72), ('SkipLayerNormalization', 48), ('Attention', 24), ('BiasGelu', 24), ('Gather', 5), ('Cast', 4), ('Add', 3), ('Equal', 2), ('Gemm', 2), ('CumSum', 1), ('Expand', 1), ('LayerNormalization', 1), ('Mul', 1), ('Not', 1), ('ReduceSum', 1), ('Shape', 1), ('Slice', 1), ('Tanh', 1), ('Unsqueeze', 1), ('Where', 1)]\n",
      "get_fused_operator_statistics: Optimized operators: {'EmbedLayerNormalization': 0, 'Attention': 24, 'MultiHeadAttention': 0, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 24, 'GemmFastGelu': 0, 'LayerNormalization': 1, 'SimplifiedLayerNormalization': 0, 'SkipLayerNormalization': 48, 'SkipSimplifiedLayerNormalization': 0, 'RotaryEmbedding': 0, 'QOrderedAttention': 0, 'QOrderedGelu': 0, 'QOrderedLayerNormalization': 0, 'QOrderedMatMul': 0}\n",
      "                main: The model has been optimized.\n",
      "  save_model_to_file: Sort graphs in topological order\n",
      "  save_model_to_file: Model saved to ./onnx_models/roberta-large-subtaskB-onnx_gpu_fp16.onnx\n"
     ]
    }
   ],
   "source": [
    "optimized_fp16_model_path = './onnx_models/roberta-large-subtaskB-onnx_{}_fp16.onnx'.format('gpu' if use_gpu else 'cpu')\n",
    "!python -m onnxruntime.transformers.optimizer --input './onnx_models/roberta-large-subtaskB-onnx-gpu.onnx' --output './onnx_models/roberta-large-subtaskB-onnx_gpu_fp16.onnx' --float16 --use_gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".onnx_multiway",
   "language": "python",
   "name": ".onnx_multiway"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
